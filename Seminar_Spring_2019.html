  <!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html" />
<link rel="stylesheet" type="text/css" href="style.css" title="generic stylesheet" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<title>Student Probability Seminar, Purdue University</title>

</script>
<script type="text/JavaScript">
     function SwitchVisibility(objId){
	var el = document.getElementById(objId);
     if (el.className == 'abstract_hide'){
        el.style.display = "none";
        el.className = 'abstract_show';
     } else {
	el.style.display = "block";
        el.className = 'abstract_hide';
	}
	}
</script>

<style type="text/css">

.leftnav {
}
.leftnavbig {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
	color: #666666;
	text-decoration: none;
}
.leftnavbigcurrent {
	font-family: Arial, Helvetica, sans-serif;
	font-size: 14px;
	color: #0099CC;
	text-decoration: none;
}
.style5 {color: #000066}
.style6 {color: #FFFFFF}
.abstract {display: none; font-size: small}


</style>

</head>


<body>


<div id="title"><center>Spring 2019 Talks</center></div>

<div id="main" align=left>


</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
Student Probability Seminar, Purdue University</p>
</center></h1>

 

<p>
Thursdays in REC 117 from 3:30-4:20pm, unless otherwise noted. 

<!--
<p>
Please contact me if you want to be added to the mailing list. 
-->

<p>
The goal of this seminar is for students (and the occasional professor) to give introductions to various areas of probability or important concepts in probability. The talks should be accessible to anyone who has any graduate-level background in probability. Talks are done with chalk and last 50 minutes. Questions are encouraged during and after the talks. 
</p>
</div>

<div id="content">
<p>
</div>

<div id="main" align=left>

<!--<h1> Schedule and Abstracts </h1>-->

<table cellpadding="2" cellspacing="10" style="width:100%">
<col width="170">
<col width="300">
<col width="680">
 
  
  <tr>
  <th align="left"><h3><u>Date</u></h3></th>
  <th align="left"><h3><u>Speaker</u></h3></th>
  <th align="left"><h3><u>Title</u></h3></th>
 </tr>
 
 <tr>
<td valign="top"><font size="4.5">1/17/2019 </font> </td>
<td valign="top"> <a href="http://www.math.purdue.edu/~dslonim/">
<font size="4.5">Daniel Slonim</a>  
<td valign="top"><font size="4">
The Prohorov Metric
</font>
<br>
<small>
<a href="javascript:SwitchVisibility('slonim')"><font size="3">Abstract</font></a>
<div class="abstract" id="slonim">
For a separable metric space $M$, the Prohorov metric is a way of quantifying how far apart two probability measures on $M$ are. The metric induces the topology of weak convergence; in other words, the set of probability measures on $M$ becomes a separable metric space in its own right, and a sequence $\{P_n\}$ of probability measures converges weakly to $P$ if and only if $P_n$ converges to $P$ under the Prohorov metric. The Prohorov metric is a generalization of the Levy metric, which applies only to $S=\mathbb{R}$. Although the definition of the Prohorov metric looks tough to swallow at first, I will try to present it in a way that makes it feel as natural as possible.
</div>
</small>
</td>
 </tr>



 
<tr>
<td valign="top"><font size="4.5">1/24/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~zselk/">
<font size="4.5">Zack Selk</a>  
<td valign="top"><font size="4">Gaussian Processes and Fractional Brownian Motion</font>
<br>
<small>
<a href="javascript:SwitchVisibility('selk')"><font size="3">Abstract</font></a>
<div class="abstract" id="selk">
Gaussian processes are a class of stochastic processes describing a wide range of phenomena. In this talk I define Gaussian process and discuss basic properties. One of the most interesting Gaussian processes is fractional Brownian motion. Fractional Brownian motion is the only centered, continuous, stationary, self similar Gaussian process. I will prove this. It finds many uses in finance, physics, biology and many other subjects. I discuss some nice properties and some not so nice properties of this process. 
</div>
</small>
</td>
 </tr>
 
 
<tr>
<td valign="top"><font size="4.5">1/31/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~pmariano/">
<font size="4.5">Phanuel Mariano</a>  
<td valign="top"><font size="4">Multiplicative Law of Large Numbers and Central Limit Theorem for Products of Random Matrices</font>
<br>
<small>
<a href="javascript:SwitchVisibility('mariano')"><font size="3">Abstract</font></a>
<div class="abstract" id="mariano">
Abstract Here
</div>
</small>
</td>
 </tr>
 

<tr>
<td valign="top"><font size="4.5">2/7/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~kim1636/">
<font size="4.5">Daesung Kim</a>  
<td valign="top"><font size="4">The Log Sobolev Inequality and the Concentration of Measure</font>
<br>
<small>
<a href="javascript:SwitchVisibility('kim')"><font size="3">Abstract</font></a>
<div class="abstract" id="kim">
In this talk, we discuss the log Sobolev inequality and its application to the concentration of measure phenomenon. I will present the proof of the inequality by L. Gross, which relies on the central limit theorem.
</div>
</small>
</td>
 </tr>

<tr>
<td valign="top"><font size="4.5">2/14/2019 </font> </td>
<td valign="top"> <a href="https://www.reddit.com/r/Purdue/comments/58fbbg/best_ma158_instructor/">
<font size="4.5">Zach Letterhos</a>  
<td valign="top"><font size="4">Generalizing the Central Limit Theorem</font>
<br>
<small>
<a href="javascript:SwitchVisibility('letterhos')"><font size="3">Abstract</font></a>
<div class="abstract" id="letterhos">
The Central Limit Theorem tells us that the sum of iid random variables with finite mean and variance, when centered and scaled appropriately, converges to a standard normal random variable. I will discuss generalizations of the CLT which relax the assumptions of identical distribution or independence. If time permits, we'll see what happens to the CLT when the assumption of finite variance is relaxed.

</div>
</small>
</td>
 </tr>

<tr>
<td valign="top"><font size="4.5">2/28/2019 </font> </td>
<td valign="top"> <a href="https://www.math.purdue.edu/people/bio/xie287">
<font size="4.5">Yongjia Xie</a>  
<td valign="top"><font size="4">A Proof of the Central Limit Theorem by Relative Entropy</font>
<br>
<small>
<a href="javascript:SwitchVisibility('xie')"><font size="3">Abstract</font></a>
<div class="abstract" id="xie">
The Central Limit Theorem has several kinds of proofs. What we learned in class is by the convergence of characteristic functions. I will show you one that is proved by relative entropy. This proof is given by Andrew R. Barron in 1986. The proof itself can also be viewed as a corollary of a more general property of the relative entropy.


</div>
</small>
</td>
 </tr>
 

 <tr>
<td valign="top"><font size="4.5">3/7/2019 </font> </td>
<td valign="top"> <a href="http://www.math.purdue.edu/~dslonim/">
<font size="4.5">Daniel Slonim</a>  
<td valign="top"><font size="4">
Beta and Dirichlet Distributions, Order Statistics, and Polya Urns
</font>
<br>
<small>
<a href="javascript:SwitchVisibility('slonim2')"><font size="3">Abstract</font></a>
<div class="abstract" id="slonim2">
In 519, we briefly learned about the density function of a beta random variable with parameters $a>0$ and $b>0$, and then tried our best to forget all about it. Recall that a beta random variable takes values between 0 and 1, but the density changes dramatically depending on how you adjust the parameters. If $a$ and $b$ are positive integers, then the beta distribution with parameters $a$ and $b$ is the conditional distribution of a probability parameter $p$ for a binomial random variable, given $a-1$ successes and $b-1$ failures in $a+b-2$ attempts and given that the prior distribution for $p$ is uniform on $[0,1]$. <p></p>
The Beta distribution can also be thought of in terms of Polya urns. Given an urn with $a$ green balls and $b$ red balls, suppose we pull out a ball at random, and then put it back with one more of the same color. Continue this process indefinitely, and the eventual proportion of green balls follows a Beta distribution with parameters $a$ and $b$. <p></p>
The connection between these two ways of looking at the Beta distribution is surprising, as the relationship is not immediately obvious, and subtracting 1 from $a$ and $b$ in the first viewpoint but not the second seems mysterious. Rather than simply proving the relationship by calculations, I will attempt to give an explanation that feels satisfying and shows a natural connection via a third viewpoint: namely, that beta distributions are the distributions of the order statistics of multiple independent uniform random variables on $[0,1]$. Dirichlet distributions are a very natural "higher-dimensional" generalization of beta distributions, and my arguments can be generalized to these with little difficulty. 
</div>
</small>
</td>
 </tr>


<tr>
<td valign="top"><font size="4.5">3/21/2019 </font> </td>
<td valign="top"> <a href="https://twitter.com/trolling_94">
<font size="4.5">Tim Rolling</a>  
<td valign="top"><font size="4">A Probabilistic Proof of the Stone-Weierstrass Theorem</font>
<br>
<small>
<a href="javascript:SwitchVisibility('rolling')"><font size="3">Abstract</font></a>
<div class="abstract" id="rolling">
In a basic analysis course, we saw a proof of the Stone-Weierstrass Theorem using the polynomials
\[P_n(x)=\int_0^1f(t)c_n(1-(x-t)^2)^ndt\]
where $c_n$ satisfies $\int_0^1c_n(1-(x-t)^2)^ndt=1$. In this talk, I will give another proof of the Stone-Weierstrass Theorem using the Weak Law of Large Numbers and Bernstein polynomials given by:
\[B_n(x)=\sum_{k=0}^nf(k/n)\binom{n}{k}x^k(1-x)^{n-k}\]
</div>
</small>
</td>
 </tr>
 
<tr>
<td valign="top"><font size="4.5">3/28/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~zselk/">
<font size="4.5">Zack Selk</a>  
<td valign="top"><font size="4">Gaussian Measure Theory in Infinite Dimensions</font>
<br>
<small>
<a href="javascript:SwitchVisibility('selk2')"><font size="3">Abstract</font></a>
<div class="abstract" id="selk2">
The archetypical infinite dimensional Gaussian measure is Wiener measure on the classical Wiener space $C_0[0,T]$. This measure is the law of Brownian motion. In this talk, we discuss how to generalize Gaussian measures on $\mathbb{R}^n$ to Gaussian measures on general Banach or even locally convex topological vector spaces. One of the most important results in infinite dimensional Gaussian measure theory is Fernique's theorem. We discuss Fernique's theorem and may or may not offer a proof, as is true in any talk.
</div>
</small>
</td>
 </tr>
 
 
<tr>
<td valign="top"><font size="4.5">4/4/2019 </font> </td>
<td valign="top"> <a href="https://www.facebook.com/BSUur/photos/a.170904099605842/1431237276905845/?type=3&theater">
<font size="4.5">Guillermo Ortiz</a>  
<td valign="top"><font size="4">Percolation</font>
<br>
<small>
<a href="javascript:SwitchVisibility('ortiz')"><font size="3">Abstract</font></a>
<div class="abstract" id="ortiz">
Percolation is one of the simplest probabilistic models that exhibits what is referred to as a critical phenomenon. In this talk we’ll take a brief exploration to the field of percolation theory. We will start by looking at the original motivation behind the field, the different variations on the model, and important results in the field. In particular, we will sketch a proof of the existence of the critical probability $p_c(d)$ in different dimensions. We will conclude the talk by discussing other known results and open problems in the field.


</div>
</small>
</td>
 </tr>
 

<tr>
<td valign="top"><font size="4.5">4/11/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~peterson/">
<font size="4.5">Jonathon Peterson</a>  
<td valign="top"><font size="4">Large Deviations and Cramer's Theorem</font>
<br>
<small>
<a href="javascript:SwitchVisibility('peterson')"><font size="3">Abstract</font></a>
<div class="abstract" id="peterson">
Abstract Here
</div>
</small>
</td>
 </tr>


<tr>
<td valign="top"><font size="4.5">4/18/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~zselk/">
<font size="4.5">Zack Selk</a>  
<td valign="top"><font size="4">Introduction to Stein's Method and Berry Esseen theorem</font>
<br>
<small>
<a href="javascript:SwitchVisibility('aprileighteenth')"><font size="3">Abstract</font></a>
<div class="abstract" id="aprileighteenth">
During a conversation with Daniel Slonim, he mentioned how much he cares about rates of convergence. Due to his enthusiasm for rates of convergence I had to indulge him. 


Stein's method is a technique for determining if a measure is close to a Gaussian. One of the big results is Berry Esseen theorem which is a quantitative version of central limit theorem. In this talk I will discuss Stein's heuristic, state and solve Stein's equation and use this to prove bounds on TV distance. 
</div>
</small>
</td>
 </tr>
 
<tr>
<td valign="top"><font size="4.5">4/25/2019 </font> </td>
<td valign="top"> <a href="http://math.purdue.edu/~dslonim/">
<font size="4.5">Daniel Slonim</a>  
<td valign="top"><font size="4">An Introduction to Random Walks in Random Environments</font>
<br>
<small>
<a href="javascript:SwitchVisibility('apriltwentyfifth')"><font size="3">Abstract</font></a>
<div class="abstract" id="apriltwentyfifth">
I shamelessly take this opportunity to practice for my advanced topics exam. 

As the name "random walks in random environments" suggests, the model contains two levels of randomness. We first consider random walks on the integers. An environment $\omega$ is an assignment of a "stepping right" probability $\omega_x$ to each integer $x$. The first level of randomness is in how the probabilities $\omega_x$ are assigned, which we assume occurs in an iid way. Once an environment is fixed, we consider a random walk on the integers where at every point $x$, the probability of stepping to the right is $\omega_x$ and the probability of stepping to the left is $1-\omega_x$. I will give a characterization of directional transience (i.e. whether the walk wanders off to infinity or negative infinity or is recurrent), and perhaps a law of large numbers with an explicit formula for limiting velocity.
<P>
My primary interest is in higher dimensional models, where the walk is on $\mathbb{Z}^d$ rather than $\mathbb{Z}$, and the transition probabilities are still chosen in an iid random way. In this setting, there is no explicit formula for limiting velocity or explicit characterization of directional recurrence and transience. In fact, the situation is even worse than this, and I will explain why. I will, however, give a couple of results about directional transience and limiting velocity that are known. Time permitting, I will also sketch a proof of a functional central limit theorem under certain conditions.
</div>
</small>
</td>
 </tr>
 

 
</table>


</div>

</body>
</html>

